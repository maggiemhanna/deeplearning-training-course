{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Brief Overview of RNN Fundamentals\n",
    "\n",
    "Before we begin learning about how to build a Recurrent Neural Network, let us quickly recap the basic definition of RNN, as we learned in an earlier blog post.\n",
    "\n",
    "Recurrent Neural Networks (RNN) are an alternative to traditional neural networks, especially used in cases where either the input data or the output data or both are sequential. By building sequence models such as RNNs, we can gain deeper insights into the problems of Speech Recognition, Machine Translation, Natural Language Processing, Music Generation, and many more.\n",
    "\n",
    "Let us, now, move forward and understand how these sequence models or Recurrent Neural Networks are built, by training them how to map the input x and output y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mapping Input and Output\n",
    "\n",
    "The first task at hand for us is to try and see how much of the solution can standard neural networks provide us in our sequential dataset problems.\n",
    "\n",
    "## 2.1. Drawbacks of Using Standard Neural Networks\n",
    "\n",
    "First, let us recall the example we used in the previous section. \n",
    "\n",
    "<img src=\"figures/rnn_word_representation.jpg\" width=\"800px\">\n",
    "          \n",
    "A standard neural network for this example will look something like this:\n",
    "\n",
    "<img src=\"figures/standard_neural_network.jpg\" width=\"600px\">\n",
    "\n",
    "In our example, there are a total of nine input words. We could use nine One-Hot Vectors for each of these nine words and, along with a few hidden layers, feed it into a standard neural network. The output of this standard neural network will be in the form of nine values, either 0 or 1, where 1 denotes the word which is part of a person’s name.\n",
    "\n",
    "However, there are a couple of problems that will arise eventually with such standard neural networks.\n",
    "\n",
    "Not all datasets we come across will have the same lengths of Input $  T_{x} $ and Output $  T_{y} $. We can try capping the input and output lengths to a set maximum value, but it will still not be a good representative solution.\n",
    "For a naïve neural network architecture, sharing features learned across different text positions is important. For example, if the neural network has learnt that the word “Luke” appearing in a certain position in a sequence is a part of a person’s name, it should also learn automatically that the word “Luke” appearing elsewhere in the sequence, say at $x^{⟨t⟩}$, is also part of a person’s name. This is quite similar to Convolutional Neural Networks, where we wanted the network to generalize results for one part of the image to other parts of the image as well. However, working with even a small 10,000-dimensional one-hot vector will make the input layer very large and we would end up having an enormous number of parameters.\n",
    "\n",
    "Recurrent Neural Networks solve the above two key problems that arise if we start to think about using Standard Neural Networks to solve our sequential data problems.\n",
    "\n",
    "We are now ready to see the framework of building a Recurrent Neural Network.\n",
    "\n",
    "\n",
    "## 2.2. The architecture of a Recurrent Neural Network\n",
    "\n",
    "Understanding why standard neural networks fail to address the problems of interpreting sequential data is important to dig deeper into the blueprint of RNNs. The advantages of using Recurrent Neural Networks are significant but first, let us understand how these RNNs are built.\n",
    "\n",
    "Let us look at the network below, where the length of the input and output sequence is the same. We will slowly move towards more scenarios and gain deeper knowledge about the benefits that RNN provides us.\n",
    "\n",
    "<img src=\"figures/rnn_model.jpg\" width=\"600px\">\n",
    "\n",
    "The first input to the first layer of the above Neural Network is $x^{⟨1⟩}$. This input goes through a hidden layer and predicts an output whether the first word is part of a person’s name or not. The role of the Recurrent Neural Network comes into play when the second word of the input sentence, $x^{⟨2⟩}$, is being read. Instead of just predicting $\\hat{y}^{⟨2⟩}$ using $x^{⟨2⟩}$, it also receives a second input in the form of an activation value from Time Step 1. The same process is repeated for the next time steps as well, until the last time step, where the input is $x^{⟨T_x⟩}$ and the output is $\\hat{y}^{⟨T_y⟩}$. For the initial time step 1, the activation value is kept at 0, which is nothing but a vector of zeros with a dummy time step 0. There are some other ways adopted by few researchers who initialize the activation value, $a^{⟨0⟩}$, randomly but having a vector of zeroes is the most common practice.\n",
    "\n",
    "Like we mentioned earlier and as you can see too that in this example, the length of the input sequence $T_x$ is equal to the length of the output sequence $T_y$ The architecture of the Recurrent Neural Network will change once the input and output sequence lengths start to differ.\n",
    "\n",
    "Apart from the above graphical representation of a Recurrent Neural Network, there is another way of representing RNN as seen in some research papers. In such representations, a loop is drawn on the time step indicating that the layer feeds back to itself.\n",
    "\n",
    "<img src=\"figures/rnn_model_2.jpg\" width=\"600px\">\n",
    "\n",
    "The above RNN reads the input data from left to right while sharing parameters at each time step. We will describe the parameters of the hidden layers in detail ahead but for now, let’s look at the parameters that govern the connection between $x^{⟨1⟩}$ and the hidden layer. This parameter, which is the same for every time step, can be denoted as $w_{aa}$ And similarly, the common parameter that governs the output predictions is denoted as $w_{ya}$\n",
    "\n",
    "<img src=\"figures/nn_model_parameters.jpg\" width=\"600px\">\n",
    "\n",
    "Consider the output parameter $\\hat{y}^{⟨3⟩}$. This output or this prediction is made not just from this time step’s input $x^{⟨3⟩}$, but also from the information received from the previous inputs $x^{⟨1⟩}$ and $x^{⟨2⟩}$ which is being passed along with activation values and the parameters governing them.\n",
    "\n",
    "<img src=\"figures/nn_model_flow.jpg\" width=\"600px\">\n",
    "\n",
    "In all the graphical representations above, we can see the flow of information is in one direction only, i.e., from left to right. Such sequence models or RNNs pose some limitations which we will learn ahead.\n",
    "\n",
    "\n",
    "## 2.3. Unidirectional Recurrent Neural Networks\n",
    "\n",
    "In our discussions above, we mentioned how information from previous inputs in the sequence is used when predicting, say, $\\hat{y}^{⟨3⟩}$. Having said that, we can also notice how it only used the previous information and doesn’t take into consideration information from the words ahead in the sequence, such as $x^{⟨4⟩}$, $x^{⟨5⟩}$,  $x^{⟨6⟩}$ and so on.\n",
    "\n",
    "Consider a new sentence: “Bear Grylls loves adventure.” For the network to judge that “Bear” is a part of a person’s name, not only will it be useful for it to have information from the first two words but the rest of the sentence as well. Why we say this is because the sentence could also have been: “Bear with me for some time please.” How is a neural network supposed to judge whether “Bear” is a part of a person’s name or just an ordinary word? In the first example, it is a name while in the second it is not.\n",
    "\n",
    "This limitation arises with the use of Unidirectional Neural Network architecture and can only be solved using Bidirectional Recurrent Neural Networks (BRNN), which modify the architecture slightly to incorporate information from all input words of the sentence at each time step. You will learn more about BRNNs in our future posts. For the sake of simplicity and understanding key concepts, we will be working with Unidirectional Recurrent Neural Networks only, for now.\n",
    "\n",
    "Now we know how a basic RNN architecture is created. Let us continue building RNNs, this time using mathematical notations, and understanding how forward propagation works in Recurrent Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Forward Propagation in RNN\n",
    "\n",
    "After a thorough understanding of the basic architectural framework of RNNs, we will learn how the whole process works and can be represented mathematically. We will start with the computational formulas used in the process of Forward Propagation in Recurrent Neural Networks (RNN).\n",
    "\n",
    "Let us look at the RNN model again.\n",
    "\n",
    "<img src=\"figures/nn_model_flow.jpg\" width=\"600px\">\n",
    "\n",
    "<img src=\"figures/rnn_model_2.jpg\" width=\"500px\">\n",
    "\n",
    "So, as we learnt earlier, the process starts off with the first ‘fake’ initial activation input, $a^{⟨0⟩}=0$ . Followed by this initial value, the subsequent forward propagation of input activation values can be written in the form of weights and biases as:\n",
    "\n",
    "$$a^{\\left \\langle 1 \\right \\rangle}= g\\left ( w_{aa}\\times a^{\\left \\langle 0 \\right \\rangle}+w_{ax}\\times x^{\\left \\langle 1 \\right \\rangle}+b_{a} \\right )$$\n",
    "\n",
    "Again, in order to compute $\\hat{y}^{(1)}$, there will be yet another activation function internally, which will have the following mathematical formula:\n",
    "\n",
    "$$\\hat{y}^{\\left \\langle 1 \\right \\rangle}= g\\left ( w_{ya}\\times a^{\\left \\langle 1 \\right \\rangle}+b_{y} \\right )$$\n",
    "\n",
    "If we look at the notation $w_{ax}$ the $x$ means that the weight is to be multiplied by $ x^{\\left \\langle 1 \\right \\rangle} $, and the $a$ means that the notation $w_{ax}$ is being used to calculate a value like $a$. In the same way, when we write $w_{ay} $, it means that it is going to be multiplied by the value $a$ and will be used to calculate the value $y$.\n",
    "\n",
    "The activation function, as mentioned above, will most commonly be a Tanh. While sometimes for RNNs, ReLu can also be used but Tanh is the preferred option to solve the vanishing gradient problem. In the case of our output $y$, choosing an activation function is an important step as well. If the output is a binary classification, we will use a Sigmoid activation function. Else, if the output is a K-classification problem, we shall go with Softmax. Therefore, for the Name Entity Recognition problem where $w$ was either $0$ or $1$, $g$ can be taken as a Sigmoid activation function. Writing in more general terms, we could denote $a^{⟨t⟩}$ at time $t$ as:\n",
    "\n",
    "$$a^{\\left \\langle t \\right \\rangle}= g\\left ( w_{aa}\\times a^{\\left \\langle t-1 \\right \\rangle}+w_{ax}\\times x^{\\left \\langle t \\right \\rangle}+b_{a} \\right )$$\n",
    "\n",
    "$$\\hat{y}^{\\left \\langle t \\right \\rangle}= g\\left ( w_{ya}\\times a^{\\left \\langle t \\right \\rangle}+b_{y} \\right )$$\n",
    "\n",
    "We can even write this Forward Propagation notation in a matrix form to arrive at a neatly stacked formula such as this:\n",
    "\n",
    "<img src=\"figures/Wa.jpg\" width=\"300px\">\n",
    "\n",
    "We could also write the above scenario in general form, by stacking the vectors $a^{⟨t−1⟩}$ and $x^{⟨t⟩}$ to form a 10,100-dimensional vector, something like this:\n",
    "\n",
    "<img src=\"figures/ax.jpg\" width=\"300px\">\n",
    "\n",
    "\n",
    "You can even validate the above equation by working backward to see if you can reach the original Forward Propagation equation using this. Let us see how that works out.\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix}w_{aa} & w_{ax}\\end{bmatrix}\\times \\begin{bmatrix}a^{\\left \\langle t-1 \\right \\rangle}\\\\x^{\\left \\langle t \\right \\rangle}\\end{bmatrix}= w_{aa}\\times a^{\\left \\langle t-1 \\right \\rangle}+w_{ax}\\times x^{\\left \\langle t \\right \\rangle}$$\n",
    "\n",
    "Now we can rewrite our original equations using the simplified notation in the following way:\n",
    "\n",
    "\n",
    "$$a^{\\left \\langle t \\right \\rangle}= g\\left ( w_{a}\\times \\left[ a^{\\left \\langle t-1 \\right \\rangle} \\ \\ x^{\\left \\langle t \\right \\rangle} \\right]+b_{a} \\right )$$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlcourse-venv",
   "language": "python",
   "name": "dlcourse-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
