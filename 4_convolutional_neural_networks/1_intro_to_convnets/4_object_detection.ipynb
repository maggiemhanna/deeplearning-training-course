{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Object Localization\n",
    "\n",
    "Object detection is one of the areas of computer vision thatâ€™s exploding and itâ€™s working so much better than just a couple years ago. In order to build up object detection we first learn about object localization. Letâ€™s start by defining what that means.\n",
    "\n",
    "We have already said that the image classification task is to look at a picture and say is there a car or not. Classification with localization means not only do we have to label an object as a car, but also to put a bounding box or draw a rectangle around the position of the car in the image. In the term classification with localization problem, localization refers to figuring out where in the picture is the car weâ€™ve detected.\n",
    "\n",
    "Weâ€™ll learn about the detection problem where there might be multiple objects in the picture and we have to detect them all and localize them all. If weâ€™re doing this for an autonomous driving application then we might need to detect not just other cars but maybe other pedestrians and motorcycles or even other objects. The classification and the classification with localization problems usually have one big object in the middle of the image that weâ€™re trying to recognize or recognize and localize. In contrast in the detection problem there can be multiple objects, and in fact maybe even multiple objects of different categories within a single image. The ideas we learn about image classification will be useful for classification with localization and then the ideas we learn for localization will turn out to be useful for detection.\n",
    "\n",
    "## 1.1. What are localization and detection ?\n",
    "\n",
    "<img src=\"figures/local_detection.png\" style=\"width:700px\">\n",
    "\n",
    "Letâ€™s start by talking about Classification with localization. Weâ€™re already familiar with the image classification problem in which we might input a picture into a ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ with multiple layers, and this results in a vector of features that is fed to maybe a ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ unit that outputs the predicted class.\n",
    "\n",
    "<img src=\"figures/convnet_softmax.png\" style=\"width:700px\">\n",
    "\n",
    "\n",
    "If weâ€™re building a self-driving car, maybe our object categories are a pedestrian, a car, a motorcycle and a background (this means none of the above). So, if thereâ€™s no pedestrian, no car, no motorcycle then we may have an output background. These are our four classes, so we need a ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ with 4 possible outputs.\n",
    "\n",
    "How about if we want to localize the car in the image as well ? To do that we can change our neural network to have a few more output units that output a bounding box. In particular, we can have the neural network output 4 more numbers, and those numbers will be $ğ‘_ğ‘¥$, $ğ‘_ğ‘¦$, $ğ‘_â„$, $ğ‘_ğ‘¤$. These 4 numbers parameterize the bounding box of the detected object.\n",
    "\n",
    "\n",
    "<img src=\"figures/localization.png\" style=\"width:700px\">\n",
    "\n",
    "## 1.2. Classification with localization\n",
    "\n",
    "Here is used the notation that the upper left point of the image is (0,0), lower right is (1,1).\n",
    "\n",
    "<img src=\"figures/bounding_box.png\" style=\"width:300px\">\n",
    "\n",
    "Specifying the bounding box the red rectangle requires specifying the midpoint, so thatâ€™s the point $ğ‘_ğ‘¥$, $ğ‘_ğ‘¦$ as well as the height that would be $ğ‘_â„$, as well as the width $ğ‘_ğ‘¤$ of this bounding box. Now if our training set contains not just the object class label, which our neural network is trying to predict up here, but it also contains 4 additional numbers giving the bounding box, then we can use supervised learning to make our algorithm outputs not just a class label, but also the 4 parameters to tell us where is the bounding box of the object we detected. In this example the $ğ‘_ğ‘¥$ might be about 0.5 because this is about half way to the right to the image, $ğ‘_ğ‘¦$ might be about 0.7 since thatâ€™s about  70 % of the way down to the image, $ğ‘_â„$ might be about 0.3 because the height of this red square is about 30 % of the overall height of the image, and $ğ‘_ğ‘¤$ might be about 0.4 because the width of the red box is about 0.4 of the overall width of the entire image.\n",
    "\n",
    "## 1.3. Defining the target label ğ‘¦\n",
    "\n",
    "Letâ€™s formalize this a bit more in terms of how we define the target label ğ‘Œ for this as a supervised learning task. Letâ€™s define the target label ğ‘Œ. Itâ€™s going to be a vector where the first component $ğ‘_ğ‘$ is going to show is there an object. If the object is a pedestrian, a car or a motorcycle, $ğ‘_ğ‘$ will be equal to 1, and if it is the background class (if itâ€™s none of the objects weâ€™re detected ), then $ğ‘_ğ‘$ will be 0. We can think $ğ‘_ğ‘$ stands for the probability that thereâ€™s a object, probability that one of the classes weâ€™re trying to detect is there, something other than the background class.\n",
    "\n",
    "Our vector ğ‘¦ would be as follows:\n",
    "\n",
    "$$y=\\begin{bmatrix} p_{c}\\\\ b_{x}\\\\b_{y}\\\\b_{h}\\\\b_{w}\\\\c_{1}\\\\c_{2}\\\\c_{3}\\end{bmatrix}$$\n",
    "\n",
    "Next, if there is an object then we want to output $ğ‘_ğ‘¥$, $ğ‘_ğ‘¦$, $ğ‘_â„$ and $ğ‘_ğ‘¤$, the bounding box for the object we detected. And finally, if there is an object, so if $ğ‘_ğ‘$=1, we want to also output ğ¶1, ğ¶2 and ğ¶3 which tells is it the ğ‘ğ‘™ğ‘ğ‘ ğ‘ 1, ğ‘ğ‘™ğ‘ğ‘ ğ‘ 2 or ğ‘ğ‘™ğ‘ğ‘ ğ‘ 3, in other words is it a pedestrian, a car or a motorcycle. We assume that our image has only one object and the most one of these objects appears in the picture in this classification with localization problem. Letâ€™s go through a couple of examples.\n",
    "\n",
    "If  ğ‘¥ is a training set image, then ğ‘¦ will have the first component $ğ‘_ğ‘$=1 because there is an object, then $ğ‘_ğ‘¥$,$ğ‘_ğ‘¦$, $ğ‘_â„$ and $ğ‘_ğ‘¤$ will specify the bounding box. So, our label training set weâ€™ll need bounding boxes in the labels.\n",
    "\n",
    "And then finally this is a car, so itâ€™s ğ‘ğ‘™ğ‘ğ‘ ğ‘ 2. ğ¶1=0 because itâ€™s not a pedestrian, ğ¶2=1 because it is a car, ğ¶3=0 since itâ€™s not a motorcycle. Among ğ¶1,ğ¶2,ğ¶3 at most one of them should be equal to 1.\n",
    "\n",
    "$$y= \\begin{bmatrix}1\\\\b_{x}\\\\b_{y}\\\\b_{h}\\\\b_{w}\\\\0\\\\1\\\\0 \\end{bmatrix}$$\n",
    "\n",
    "What if thereâ€™s no object in the image? In this case $ğ‘_ğ‘$=0 and the rest of the elements of this vector can be any number, because if there is no object in this image then we donâ€™t care what bounding box of the neural network outputs as well as which of the three objects ğ¶1,ğ¶2,ğ¶3 it thinks of this. \n",
    "\n",
    "$$y=\\begin{bmatrix}0\\\\?\\\\?\\\\?\\\\?\\\\?\\\\?\\\\? \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "<img src=\"figures/examples_training.png\" style=\"width:600px\">\n",
    "\n",
    "Given a set of labeled training examples this is how we construct it: ğ‘¥ the input image as well as ğ‘¦ the class label both images where there is an object and for images where there is no object, and the set of these will then define our training set.\n",
    "\n",
    " \n",
    "## 1.4. Loss function\n",
    "\n",
    "Finally letâ€™s describe the loss function we use to train the neural network. The ground truth label was ğ‘¦, and neural network outputs some ğ‘¦Ì‚   what should the loss bee.\n",
    "\n",
    "$$L\\left ( \\hat{y},y \\right )=\\left ( \\hat{y}_{1}-y_{1} \\right )^{2}+\\left ( \\hat{y}_{2}-y_{2} \\right )^{2}+â€¦+\\left ( \\hat{y}_{8}-y_{8} \\right )^{2},   y_{1}=1$$\n",
    "\n",
    "$$L\\left ( \\hat{y},y \\right )= \\left ( \\hat{y}_{1}-y_{1} \\right )^{2}, y_{1}=0$$\n",
    "\n",
    "\n",
    "Notice that ğ‘¦ here has 8 components (the first row of loss function), so that goes from sum of the squares of the difference of the elements, and thatâ€™s the loss if  ğ‘¦1=1. Thatâ€™s the case where there is an object so ğ‘¦1=$ğ‘_ğ‘$. So, if there is an object in the image, then the loss can be the sum of squares over all the different elements.\n",
    "\n",
    "The other case is if ğ‘¦1=0. Thatâ€™s if this $ğ‘_ğ‘$=0, in that case the loss can be just $\\hat{ğ‘¦_1} - ğ‘¦_1$ squared because in that second case all the rest of the components are not important. All we care about is how accurately is the neural network outputting $ğ‘_ğ‘$ in that case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Landmark Detection\n",
    "\n",
    "In the previous section we saw how we can get a neural network to output 4 numbers: $ğ‘_ğ‘¥$, $ğ‘_ğ‘¦$ ,$ğ‘_â„$, and $ğ‘_ğ‘¤$ to specify the bounding box of an object we want neural network to localize. In more general cases we can have a neural network which outputs just ğ‘¥ and ğ‘¦ coordinates of important points in the image, sometimes called landmarks. \n",
    "\n",
    "Letâ€™s see a few examples. Letâ€™s say weâ€™re building a face recognition application, and for some reason we want the algorithm to tell us where is the corner of someoneâ€™s eye.\n",
    "\n",
    "<img src=\"figures/landmark_detection.png\" style=\"width:600px\">\n",
    "\n",
    "\n",
    "Every point has an ğ‘¥ and ğ‘¦ coordinate so we can just have a neural network with final layer that outputs two more numbers which we will call ğ‘™ğ‘¥ and ğ‘™ğ‘¦ to specify the coordinates of a point that is for example the personâ€™s eye).\n",
    "\n",
    "Now, what if we wanted the neural network to tell us all four corners of the eye, or both eyes. If we call the points the first, the second, the third and fourth point, going from left to right, then we can modify the neural network to output ğ‘™1ğ‘¥, ğ‘™1ğ‘¦, for the first point, and ğ‘™2ğ‘¥, ğ‘™1ğ‘¦ for the second point and so on. The neural network can output the estimated position of all those four points of the personâ€™s face. What if we donâ€™t want just those four points? What if we want the output many points? For example what if we want to output different positions in the eye or shape of the mouth to see weather the person is smiling or not. We could define some number, for the sake of argument, letâ€™s say 64 points or 64 landmarks on the face maybe even some points that helps us define the edge of the face, it defines the jawline. By selecting a number of landmarks and generating a label training set that contains all of these landmarks we can then have the neural network which tell us where are all the key positions or the key landmarks on a face.\n",
    "\n",
    "<img src=\"figures/landmark_detection_convnet.png\" style=\"width:700px\">\n",
    "\n",
    "So, what we do is we have this image of personâ€™s face as input, have it go through a ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ and have a ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ then have some set of features maybe have it output 0 or 1, like is there a face in this or not, and then have it also output ğ‘™1ğ‘¥, ğ‘™1ğ‘¦ and so on down to ğ‘™64ğ‘¥, ğ‘™64ğ‘¦. We use ğ‘™ to stand for a landmark.\n",
    "\n",
    "This example would have 129 output units, 1 is for where a face or not, and then if we have 64 landmarks that is 64Ã—2 which is equal to 128 plus 1  output units. This can tell us if thereâ€™s a face as well as where are all the key landmarks on the face. Of course in order to trade a network like this we will need a label training set. We have a set of images as well as labels ğ‘Œ,  where someone would have had to go through and laboriously annotate all of these landmarks.\n",
    "\n",
    "## Pose detection\n",
    "\n",
    "If we are interested in personâ€™s pose detection, we could also define a few key positions (as we can see in the picture below) like the midpoint of the chest, left shoulder, left elbow, wrist and so on. Then we need a neural network to annotate key positions in the personâ€™s pose as well. Having a neural network output all of those points down annotating we could also have the neural network output the pose of the person.\n",
    "\n",
    "<img src=\"figures/pose_detection.png\" style=\"width:300px\">\n",
    "\n",
    "To do that we also need to specify on these key landmarks which may be ğ‘™1ğ‘¥, ğ‘™1ğ‘¦ that is the midpoint of the chest, down to maybe ğ‘™32ğ‘¥, ğ‘™32ğ‘¦, if we use 32 coordinates to specify the pose of the person.\n",
    "\n",
    "This idea might seem quite simple of just adding a bunch of output units to output the (ğ‘¥,ğ‘¦) coordinates of different landmarks we want to recognize. To be clear, the identity of landmark 1 must be consistent across different images like maybe landmark 1 is always one corner of the eye, landmark 2 is always another corner of the same eye etc. The labels have to be consistent across different images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Object Detection\n",
    "\n",
    "We have learned about object localization as well as landmark detection, now letâ€™s build an object detection algorithm. In this post weâ€™ll learn how to use a ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ to perform object detection using a Sliding windows detection algorithm.\n",
    "\n",
    "Letâ€™s say we want to build a car detection algorithm.\n",
    "\n",
    "<img src=\"figures/object_detection_training.png\" style=\"width:700px\">\n",
    "\n",
    "We can first create a labeled training set (ğ‘¥,ğ‘¦) with closely cropped examples of cars and some other pictures that arenâ€™t pictures of cars. For making a training dataset we can take a picture and crop it out. We want to cut out anything else that is not a part of a car, so we end up with a car centered in pretty much the entire image. Given this labeled training set we can then train a ğ¶ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ that inputs an image, like one of these closely cropped images above, and then the job of the ğ¶ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ is to output ğ‘¦ (0 or 1 is as a car or not).\n",
    "\n",
    "Once we have trained up this ğ¶ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ we can then use it in sliding windows detection. The way we do that is, if we have a test image like the following one, that we start by picking a certain window size shown down there, and then we would input into a ğ¶ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ this small rectangular region.\n",
    "\n",
    "<img src=\"figures/sliding_window.png\" style=\"width:300px\">\n",
    "\n",
    "\n",
    "Take just this little red square, as we draw in the picture above, and put that into the ğ¶ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡, and have a ğ¶ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ make a prediction. Presumably for that little region in the red square, it will say that a little red square does not contain a car. In the sliding windows detection algorithm, what we do is we then process input a second image now bounded by the red square shifted a little bit over and feed that to the ğ¶ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ we speed in just the region of the image in the red square to the ğ¶ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ and run the ğ¶ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ again, and then we do that with a third image and so on and we keep going until we split the window across every position in the image. We basically go through every region of this size and pass lots of little crafted images into the ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ and have it classify 0 or 1 for each position at some stride.Running this was called a sliding window through the image and  weâ€™d then repeat this but now using a larger window and then a more large window (as we can see in the following image).\n",
    "\n",
    "<img src=\"figures/sliding_window_sizes.png\" style=\"width:600px\">\n",
    "\n",
    "So, we take a slightly larger region and run that region, feed that to the ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ and have it output 0 or 1. Then we slide the window over again using some stride and so on, and we run that throughout our entire image until we get to the end. Then we might do the third time using even larger windows and so on.\n",
    "\n",
    "The hope is that if thereâ€™s a car somewhere in the image so, that there will be a window  where for which the ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡  will have output 1 for that input region  which means that there is a car. This algorithm is called Sliding windows detection because we take these windows, these square red boxes, and slide them across the entire image and classify every square region with some stride as containing a car or not.\n",
    "\n",
    "## Disadvantages of sliding window detection and how to overcome them\n",
    "\n",
    "Thereâ€™s a huge disadvantage of sliding windows detection which is the **Computational cost**, because weâ€™re cropping out so many different square regions in the image and running each of them independently through a ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡. If we use the very course stride, a very big stride, very big step size, then that would reduce the number of windows we need to pass through the ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡, but that coarser granularity may hurt performance, whereas if we use a very fine granularity or a very small stride then the huge number of all these little regions weâ€™re passing through the ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ means that thereâ€™s a very high computational cost. Before the rise of neural networks people used to use much simpler classifiers, like a simple linear classifier overhand engineer features in order to perform object detection, and in that error because each classifier was relatively cheap to compute it was just a linear function, sliding windows detection ran properly, it was not a bad method, but with ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ğ‘  now running a single classification task is much more expensive and sliding windows this way is infeasible slow. Unless we use a very fine granularity or a very small stride we end up not able to localize the objects that accurately within the image as well.\n",
    "\n",
    "Fortunately, this problem of computational cost has a pretty good solution. In particular the sliding windows object detector can be implemented convolutionally or much more efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Convolutional operation of sliding windows\n",
    "\n",
    "\n",
    "In the previous section we learned about the sliding windows object detection algorithm using a ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡, but we saw that it was too slow. In this post we will see how to implement that algorithm convolutionaly. Letâ€™s see what that means.\n",
    "\n",
    "To build up the convolutional implementation of sliding windows, letâ€™s first see how we can turn ğ¹ğ‘¢ğ‘™ğ‘™ğ‘¦ ğ‘ğ‘œğ‘›ğ‘›ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ layers in our neural network into ğ¶ğ‘œğ‘›ğ‘£ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ğ‘™ layers. Letâ€™s say that our object detection algorithm inputs 14Ã—14Ã—3 images, this is quite small but we will use it just for illustrative purposes, and letâ€™s say it then uses 5Ã—5 filters and letâ€™s say that it uses 16 of them to map it from 14Ã—14Ã—3 to 10Ã—10Ã—16, and we apply 2Ã—2  ğ‘€ğ‘ğ‘¥ ğ‘ğ‘œğ‘œğ‘™ğ‘–ğ‘›ğ‘” layer to reduce the size of a volume to 5Ã—5Ã—16. Then we have a ğ¹ğ‘¢ğ‘™ğ‘™ğ‘¦ğ‘ğ‘œğ‘›ğ‘›ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ layer, with 400 units, then another ğ¹ğ‘¢ğ‘™ğ‘™ğ‘¦ ğ‘ğ‘œğ‘›ğ‘›ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ layer (also with a 400) units and then a neural network finally outputs ğ‘Œ using a ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ unit.\n",
    "\n",
    "\n",
    "<img src=\"figures/cnn_example.png\" style=\"width:700px\">\n",
    "\n",
    "\n",
    "## 4.1. How to turn ğ¶ğ‘œğ‘›ğ‘£ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ğ‘™ layers into ğ¹ğ‘¢ğ‘™ğ‘™ğ‘¦ ğ¶ğ‘œğ‘›ğ‘›ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ layers ?\n",
    "\n",
    "The ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ is the same as before for the first few layers, and now one way of implementing the first ğ¹ğ‘¢ğ‘™ğ‘™ğ‘¦ğ‘ğ‘œğ‘›ğ‘›ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ layer, is to implement a 5Ã—5 filter and letâ€™s use 400  5Ã—5 filters (see the picture below). So, we take a 5Ã—5Ã—16 volume and convolve it with a 5Ã—5  filter. Remember that a 5Ã—5 filter is implemented as a 5Ã—5Ã—16 filter because our convention is that the filter looks across all 16 channels. So, if we have 400 of these 5Ã—5Ã—16 filters, then the output dimension is going to be 1Ã—1Ã—400. Rather than viewing these 400 as just a set of nodes (units), weâ€™re going to view this as a 1Ã—1Ã—400 volume and mathematically this is the same as a ğ¹ğ‘¢ğ‘™ğ‘™ğ‘¦ğ‘ğ‘œğ‘›ğ‘›ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ layer because each of these 400 nodes has a filter of dimension 5Ã—5Ã—16, so each of those 400 values is some arbitrary linear function of these 5Ã—5Ã—16 activations from the previous layer.\n",
    "\n",
    "<img src=\"figures/fully_connected_to_conv.png\" style=\"width:700px\">\n",
    "\n",
    "Next, to implement the next convolutional layer, weâ€™re going to implement a 1Ã—1 convolution, and if we have 400  1Ã—1 filters then the next layer will again be 1Ã—1Ã—400, so that gives us this next ğ¹ğ‘¢ğ‘™ğ‘™ğ‘¦ ğ‘ğ‘œğ‘›ğ‘›ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ layer. And finally weâ€™re going to have another 1Ã—1 filter followed by a ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ activation so as to give a 1Ã—1Ã—4 volume to take the place of these four numbers that the network was outputting. This shows how we can take these ğ¹ğ‘¢ğ‘™ğ‘™ğ‘¦ ğ‘ğ‘œğ‘›ğ‘›ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ layers and implement them using ğ¶ğ‘œğ‘›ğ‘£ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ğ‘™ layes. These sets of units instead are now implemented as 1Ã—1Ã—400 and 1Ã—1Ã—4 volumes.\n",
    "\n",
    "## 4.2. A convolutional implementation of sliding windows object detection\n",
    "\n",
    "Letâ€™s say that our sliding windows ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ inputs 14Ã—14Ã—3 images. As before we have a neural network as follows that eventually outputs a 1Ã—1Ã—4 volume which is the output of our ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ unit. We can see the implementation of this neural network in the following picture. \n",
    "\n",
    "<img src=\"figures/fully_connected_to_conv_2.png\" style=\"width:700px\">\n",
    "\n",
    "Letâ€™s say that our ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ inputs 14Ã—14 images or 14Ã—14Ã—3 images and our test set image is 16Ã—16Ã—3 , so now will add that yellow stripe to the border of this image as we can see in the picture below.\n",
    "\n",
    "<img src=\"figures/fully_connected_to_conv_3.png\" style=\"width:700px\">\n",
    "\n",
    "In the original sliding windows algorithm we might want to input the blue region into a ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ and run that once to generate a classification(to output 0 or 1) and then slide it down a bit, letâ€™s use the stride of 2 pixels, and then we might slide that to the right (for example we can use a stride of 2 pixels ) to input this green rectangle into the ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ and rerun the whole ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ and get another label 0 or 1.Then we might input this orange region into the ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ and run it one more time to get another label and then do the fourth and final time with this lower right now purple square. To run sliding windows on this 16Ã—16Ã—3 image, this pretty small image, we run this ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ from above 4 times in order to forget 4 labels. It turns out a lot of this computation done by these 4 ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ğ‘  is highly duplicated, so what the convolutional implementation of sliding windows does is it allows these 4 forward passes of the ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ to share a lot of computation. Specifically, hereâ€™s what we can do. We can take the convent and just run it same parameters, the same 16 5Ã—5 filters and run it, and now we can have a 12Ã—12Ã—16 output volume, and then do the max pool same as before, now we have a 6Ã—6Ã—16, run through our same 400  5Ã—5 filters to get 2Ã—2Ã—400 volume. Now instead of a 1Ã—1Ã—400 volume, we have instead a 2Ã—2Ã—400 volume. Run it through our 1Ã—1 filter and it gives us another 2Ã—2Ã—400 instead of 1Ã—1Ã—400, we will do that one more time and now we have a 2Ã—2Ã—4 output volume instead of 1Ã—1Ã—4. It turns out that this blue 1Ã—1Ã—4 subset gives us the result of running in the upper left-hand corner 14Ã—14 image, this upper right 1Ã—1Ã—4 volume gives us the upper right result, the lower left gives us the results of implementing the content on the lower left 14Ã—14 region, and the lower right 1Ã—1Ã—4 volume gives us the same result as running the ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ on the lower right 14Ã—14 region.\n",
    "\n",
    "If we step through all the steps of the calculation, letâ€™s look at the green example. If we had cropped out just this region and passed it through the ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ on top, then the first layers activations would have been exactly this region, the next layers activation of the max pooling would have been exactly this region, and then the next layer, the next layer would have been as follows. What this process does, what this convolutional inclination does, is instead of forcing us to run 4 propagation on 4 subsets of the input image independently, instead it combines all 4 into 1 for computation and shares a lot of the computation in the regions of the image that are common, all four of the 14Ã—14 patches we saw here.\n",
    "\n",
    "Letâ€™s go through a bigger example. Letâ€™s say we now want to run sliding windows on a 28Ã—28Ã—3 image. It turns out if we run for crop the same way, then we end up with an 8Ã—8Ã—4 output and this corresponds to running sliding windows with that 14Ã—14 region, and that corresponds to running sliding windows first on that region does giving us the output corresponding on the upper left-hand corner, then using stride of 2 to shift one window over, one window over, one window over and so on, there are 8 positions, so that gives us this first row. Then as we go down the image as well that gives us all of these 8Ã—8Ã—4 outputs. And because of the max pooling of 2 that this corresponds to running our neural network with a stride of 2 on the original image.\n",
    "\n",
    "<img src=\"figures/fully_connected_to_conv_bigger.png\" style=\"width:800px\">\n",
    "\n",
    "\n",
    "To recap, to implement sliding windows, previously what we do is we drop out a region, letâ€™s say this is on 14Ã—14, and run that to our convent and do that for the next region over, then do that for the next 14Ã—14 region, then the next one, then the next one, the next one, the next one and so on until hopefully that one recognizes the car. But now instead of doing it sequentially, with this convolutional implementation that we saw in the previous slide, we can implement the entire image of maybe 28Ã—28 and convolutionaly make all the predictions at the same time by one for pass through this big ğ‘ğ‘œğ‘›ğ‘£ğ‘›ğ‘’ğ‘¡ in hope it recognize the position of the car.\n",
    "\n",
    "\n",
    "<img src=\"figures/full_sliding_window_example.png\" style=\"width:800px\">\n",
    "\n",
    "Thatâ€™s how we implement sliding windows convolutionally, and it makes the whole thing much more efficient. This algorithm still has one weakness which is the position of the bounding boxes is not going to be too accurate. In the next lecture letâ€™s see how we can fix that problem.\n",
    "\n",
    "This algorithm for object detection is computationally efficient but is not the most accurate one. In the next post, we will see how we can detect objects more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. YOLO: Bounding Box Predictions \n",
    "\n",
    "In the last sections, we learned how to use a convolutional implementation of sliding windows. Thatâ€™s more computationally efficient, but it still has a problem of not outputting the most accurate bounding boxes. \n",
    "In this post, we will see how we can obtain more accurate predictions of bounding boxes.\n",
    "\n",
    "## 5.1. Output accurate bounding boxes\n",
    "\n",
    "With sliding windows, we take the sets of windows that we move throughout the image and we obtain a set of sliding windows (the purple box). The next thing we will do is applying a classifier to see if there is a car in that particular sliding window or not.  \n",
    "\n",
    "This is not the most accurate way of getting bounding boxes. Letâ€™s see what we can do. \n",
    "\n",
    "A good way to get the more accurate output bounding boxes is with the ğ‘Œğ‘‚ğ¿ğ‘‚ algorithm. ğ‘Œğ‘‚ğ¿ğ‘‚ stands for â€“ ğ‘Œğ‘œğ‘¢ğ‘‚ğ‘›ğ‘™ğ‘¦ğ¿ğ‘œğ‘œğ‘˜ğ‘‚ğ‘›ğ‘ğ‘’.\n",
    "\n",
    "## 5.2. ğ‘Œğ‘‚ğ¿ğ‘‚ algorithm\n",
    "\n",
    "<img src=\"figures/yolo_algo.png\" style=\"width:400px\">\n",
    "\n",
    "\n",
    "Letâ€™s say we have a 100Ã—100  input image. Weâ€™re going to place down a grid on this image and for the purpose of illustration. We are going to use a 3Ã—3 grid. In the actual implementations in practice, we would use a finer one, for example, a 19Ã—19 grid.\n",
    "\n",
    "We can say that the basic idea of the ğ‘Œğ‘œğ‘™ğ‘œ algorithm is applying both the image classification and localization algorithm on each of nine grid cells. \n",
    "\n",
    "### How do we define labels ğ‘¦?\n",
    "\n",
    "In the following picture, we can see what are the output vectors ğ‘¦ for the tree grid cells that are in the purple, green and orange rectangle.\n",
    "\n",
    "\n",
    "<img src=\"figures/yolo_label.png\" style=\"width:500px\">\n",
    "\n",
    "Our first output $ğ‘_ğ‘$ is either 0 or 1 depending on whether or not there is an object in that grid cell. Then, we have $ğ‘_ğ‘¥$,$ğ‘_ğ‘¦$,$ğ‘_â„$,$ğ‘_ğ‘¤$ to specify the bounding box of an object (in case that there is an object associated with that grid cell). We take ğ‘1,ğ‘2,ğ‘3 to denote if we had recognized pedestrianâ€™s class, motorcycles and the background class. So, ğ‘1,ğ‘2,ğ‘3 are labels for the pedestrian, car and motorcycle classes. \n",
    "\n",
    "In this image, we have nine grid cells, so for each grid cell, we can define a vector, like the one we saw in the picture above. Letâ€™s start with the upper left grid cell. For this grid cell, we see that there is no object present. So, the label vector ğ‘¦ for the upper left grid cell will have $ğ‘_ğ‘$=0, and then we donâ€™t care what the remaining values in this vector are. The output label ğ‘¦ would be the same for the first tree grid cells because all these tree grid cells donâ€™t have an interesting object in them.\n",
    "\n",
    "Subsequently, this analyzed image has two objects which are located in the remaining six grid cells. And what the ğ‘Œğ‘‚ğ¿ğ‘‚ algorithm does, it takes the midpoint of each of the two objects and then assigns the object to the grid cell that contains the midpoint. So, the left car is assigned to the green grid cell, whereas the car on the right is assigned to the orange grid cell. \n",
    "Even though four grid cells (bottom right) have some parts of the right car, the object will be assigned to just one grid cell. So, for the central grid cell, the vector ğ‘¦ also looks like a vector with no object. The first component $ğ‘_ğ‘$ is equal to 0, and then the rest values in this vector can be of any value. We donâ€™t care about it. Hence, for these two grid cells this we have the following vector ğ‘¦:\n",
    "\n",
    "$$y = \\begin{bmatrix} 0 \\\\ ? \\\\ ? \\\\ ? \\\\ ? \\\\ ? \\\\ ? \\\\ ?  \\end{bmatrix}$$\n",
    "\n",
    "On the other hand, for the cell circled in green on the left, the target label ğ‘¦ will be defined in the following way. First, there is an object, so $ğ‘_ğ‘$=1, and then we write $ğ‘_ğ‘¥$,$ğ‘_ğ‘¦$,$ğ‘_â„$,$ğ‘_ğ‘¤$ to specify the position of that bounding box. If class one was to mark a pedestrian then: ğ¶1=0, class two was a car ğ¶2=1 and class three was a motorcycle, so ğ¶3=0. Similarly, for the grid cell on the right, there is an object in it and this vector will have the same structure as the previous one. \n",
    "\n",
    "Finally, for each of these nine grid cells, we end up with eight-dimensional output vectors. And because we have 3Ã—3  grid cells, we have nine grid cells, the total volume of the output is going to be 3Ã—3Ã—8. So, for each of the 3Ã—3 grid cells, we have a eight-dimensional ğ‘¦ vector. \n",
    "\n",
    "<img src=\"figures/yolo_output.png\" style=\"width:200px\">\n",
    "\n",
    "The target output volume is 3Ã—3Ã—8. Where for example, this 1Ã—1Ã—8 volume in the upper left corresponds to the target output vector for the upper left of the nine grid cells. For each of the 3Ã—3 positions, for each of these nine grid cells, we have eight-dimensional target vector ğ‘¦ that we want to output. Some of which could be vectors that correspond to the cells without an object of importance, if thereâ€™s no object in that grid cell. Therefore, the total target output is a 3Ã—3Ã—8 volume. \n",
    "\n",
    "### Letâ€™s now see in more details how do we define the output vector ğ‘¦\n",
    "\n",
    "First, to train our neural network, the input is 100Ã—100Ã—3 dimensional. Then, we have a usual convolutional neural network with ğ‘ğ‘œğ‘›ğ‘£ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ğ‘™ layers, ğ‘€ğ‘ğ‘¥ğ‘ğ‘œğ‘œğ‘™ layers, and so on.  So, this neural network maps from an input image to a 3Ã—3Ã—8 output volume. \n",
    "We have an input ğ‘¥ which is the input image like this one in the picture above, and we have these target labels ğ‘¦ which are 3Ã—3Ã—8. Further, we use backpropagation to train the neural network in order to map an input ğ‘¥ to this type of output volume ğ‘¦. \n",
    "\n",
    "The advantage of this algorithm is that the neural network outputs precise bounding boxes. At the test time, we feed an input image ğ‘¥ and run forward propagation step until we get the output ğ˜.Next, for each of the nine outputs, we can read 1 or 0. That is if there is an object is some of those nine positions? \n",
    "As long as we donâ€™t have more than one object in each grid cell, this algorithm should work properly. The problem of having multiple objects within the grid cell is something weâ€™ll talk about later. \n",
    "Here we have used a relatively coarse 3Ã—3 grid, in practice, we might use a much finer grid maybe 19Ã—19. In that case we end up with 19Ã—19Ã—8 output. This step reduces the probability that we encounter multiple objects assigned to the same grid cell.  \n",
    "Letâ€™s notice two things:\n",
    "\n",
    "This algorithm resembles the image classification and localization algorithm that we explained in our previous posts. And that it outputs the bounding boxâ€™s coordinates explicitly. This allows our network to output bounding boxes different aspect ratio providing more precise coordinates in contrast to the sliding windows classifier\n",
    "This is a convolutional implementation because weâ€™re not assessing this algorithm nine times on the 3Ã—3 grid or  361 times if we are using the 19Ã—19 grid. Instead, this is one single convolutional evaluation, and thatâ€™s why this algorithm is very efficient.\n",
    " ğ‘Œğ‘‚ğ¿ğ‘‚ algorithm gained a lot of popularity because of a convolutional implementation that can detect objects even in the real-time scenarios.\n",
    "\n",
    "Last but not least, before wrapping up, thereâ€™s one more detail: how do we encode these bounding boxes $ğ‘_ğ‘¥$,$ğ‘_ğ‘¦$,$ğ‘_â„$,$ğ‘_ğ‘¤$ ? \n",
    "\n",
    "Letâ€™s take the example of the car in the picture.\n",
    "\n",
    "<img src=\"figures/yolo_bounding_box.png\" style=\"width:700px\">\n",
    "\n",
    "In this grid cell there is an object and the target label ğ‘¦will have $ğ‘_ğ‘$ equal to one. Then we have some values for $ğ‘_ğ‘¥$,$ğ‘_ğ‘¦$,$ğ‘_â„$,$ğ‘_ğ‘¤$, and the last three values in this output vector are 0,1,0 because in this cell we have recognized the car, so the class two or ğ¶2 is equal to 1. \n",
    "\n",
    "So, how do we specify the bounding box? In the ğ‘Œğ‘‚ğ¿ğ‘‚ algorithm we take the convention that the upper left point is (0,0) and this lower right point is (1,1). To specify the position of the midpoint, that orange dot in the picture above, $ğ‘_ğ‘¥$ might be 0.4 (we are looking the x-axis) because maybe itâ€™s about 0.4 of the way to the right. ğ‘¦, looks maybe like it is 0.3 (if we are in the direction of the y-axis). Next, the height of the bounding box is specified as a fraction of the overall width of this box. \n",
    "\n",
    "The width of this red box in the picture above is maybe 90% of the height of the grid cell and thatâ€™s why $ğ‘_â„$ is 0.9 and the height of this bounding box is maybe one half of the overall height of the grid cell. So, in that case, $ğ‘_ğ‘¤$, would be 0.8. In other words, this $ğ‘_ğ‘¥$,$ğ‘_ğ‘¦$ was specified relative to the grid cell. $ğ‘_ğ‘¥$ and $ğ‘_ğ‘¦$ , has to be between 0 and 1. Because pretty much by definition that orange dot is within the bounds of that grid cell to which it is assigned to. If it wasnâ€™t between 0 and 1 than it was outside the square that means that it is assigned to another grid cell. \n",
    "These could be greater than 1 in case we have a car which is in two grid cells.\n",
    "\n",
    "Although there are multiple ways of specifying the bounding boxes, this convention can be quite a reasonable one. \n",
    "In the ğ‘Œğ‘‚ğ¿ğ‘‚ research papers, there were other parameterizations that work even a little bit better, but we hope this gives one reasonable condition that should work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Intersection over Union\n",
    "\n",
    "When doing the object detection our task is to localize the object in the best possible way. Take a look at the picture below we can see that there are two bounding boxes â€“ a red one which is the ground truth bounding box and the purple one which is the output of our algorithm. We can see that they donâ€™t overlap perfectly, so we need to measure how bad ( or how good) is the actual outcome. To do that we will compute the intersection over union.\n",
    "\n",
    "In the object detection task, our expectation is to localize the object in the best possible way. Letâ€™s have a look at the image above. If the red bounding box is the ground truth bounding box (where the car is in the image) and our algorithm outputs the bounding box in purple, the intersection over union tells us whether we have a good or a bad outcome.\n",
    "\n",
    "<img src=\"figures/IoU.png\" style=\"width:500px\">\n",
    "\n",
    "The union of these two bounding boxes is a blue area. That is the area that is contained in both bounding boxes, whereas the intersection of the boxes is a smaller yellow region. The intersection over union computes the size of the intersection and divides it by the size of the union. By convention the bounding box is correct if the ğ¼ğ‘œğ‘ˆ is greater than 0.5. If the bounding box we got and the ground truth bounding boxes overlapped perfectly, the ğ¼ğ‘œğ‘ˆ would be 1 because the intersection would be equal to the union. In general as long as ğ¼ğ‘œğ‘ˆ is greater than or equal to 0.5 then the obtained answer is rather decent. By convention, 0.5 is used as a threshold to determine whether the predicted bounding box is correct or not.\n",
    "\n",
    "This is just a convention used in practice. In case that we want to be more strict, we can judge an answer as correct only if the ğ¼ğ‘œğ‘ˆ is greater than and equals to 0.6 or some other number. However, the higher the ğ¼ğ‘œğ‘ˆ is, the more accurate the bounding box is. We defined ğ¼ğ‘œğ‘ˆ as a way to evaluate whether or not our object localization algorithm is accurate or not, but more generally ğ¼ğ‘œğ‘ˆ is a measure of the overlap between two bounding boxes, where if we have two boxes, we can compute their intersection and union and then we take the ratio of these two areas. This is also a way of measuring how similar two boxes are to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Non-Max Suppression\n",
    "\n",
    "In this section, we will learn how the non-max suppression algorithm allows us to overcome multiple detections of the same object in an image. Letâ€™s go through an example! Letâ€™s say we want to detect pedestrians, cars, and motorcycles in this image.\n",
    "\n",
    "<img src=\"figures/non_max_suppression.png\" style=\"width:300px\">\n",
    "\n",
    "If we look at the picture above we can see that there are two cars. Each of these two cars has one midpoint so it should be assigned to just one grid cell which then actually predicts that there is a car in the picture. In practice, weâ€™re running an object classification and localization algorithm for every one of these grid cells, so itâ€™s quite possible that different cells might think that the center of a car is in many different cells.\n",
    "\n",
    "> ğ‘ğ‘œğ‘›âˆ’ğ‘šğ‘ğ‘¥ ğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘œğ‘›  cleans up these multiple bounding boxes\n",
    "\n",
    "Letâ€™s see an example of how ğ‘ğ‘œğ‘›âˆ’ğ‘€ğ‘ğ‘¥ ğ‘ ğ‘¢ğ‘ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘› works. Since we are running the image classification and localization algorithm on every grid cell, it is possible that many of them will be with a large probability $ğ‘_ğ‘$, that there is an object in that cell. When we run the algorithm we would end up with multiple detections of each object. What the ğ‘ğ‘œğ‘›âˆ’ğ‘€ğ‘ğ‘¥ ğ‘ ğ‘¢ğ‘ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘› does is it bends together these detections so that we end up with just one detection per car.\n",
    "\n",
    "<img src=\"figures/non_max_suppression_2.png\" style=\"width:400px\">\n",
    "\n",
    "More specifically, this algorithm will first search for the probability associated with each of these detections, so it looks at $ğ‘_ğ‘$ values first, and then it takes the largest one. In the picture above, there is a rectangle associated with 0.9 (the car on the right) and this means that an algorithm has detected a car there. After this, the ğ‘ğ‘œğ‘›âˆ’ğ‘€ğ‘ğ‘¥ ğ‘ ğ‘¢ğ‘ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘› looks at other rectangles that are close to the first one and the ones with the highest overlap with this one (highest ğ¼ğ‘œğ‘ˆ) will be suppressed. As an example, in the picture above we can see those two rectangles with the ğ¼ğ‘œğ‘ˆ 0.6 and the 0.7 are going to be suppressed.\n",
    "\n",
    "More specifically, it first searches for the probability associated with each of these detections (it looks at $ğ‘_ğ‘$ values) and first, it takes the largest one. In the picture above, there is a rectangle associated with 0.9 (the car on the right). This means that an algorithm detected a car there. After this, the ğ‘ğ‘œğ‘›âˆ’ğ‘€ğ‘ğ‘¥ ğ‘ ğ‘¢ğ‘ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘› looks at other rectangles that are close and the ones with a high overlap with this one (high ğ¼ğ‘œğ‘ˆ) will be suppressed. As an example, we can see those two rectangles with the ğ¼ğ‘œğ‘ˆ 0.6 and the 0.7 are going to be suppressed.\n",
    "\n",
    "Similarly, the same procedure can be applied for the car on the left, we go through the remaining rectangles and find the one with the highest probability, the highest $ğ‘_ğ‘$ value. So for the car on the left, it will be the box with the value $ğ‘_ğ‘$=0.8. Next, a task of ğ‘ğ‘œğ‘›âˆ’ğ‘€ğ‘ğ‘¥ ğ‘ ğ‘¢ğ‘ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘› is to discard the remaining detections having high ğ¼ğ‘œğ‘ˆ with this cell. In this case, we have two bounding boxes with lower $ğ‘_ğ‘$ that recognized the car on the left (two red rectangles in the left in the above picture) and because they both have big ğ¼ğ‘œğ‘ˆ both of them will be discarded.\n",
    "\n",
    "Similarly, for the car on the left, we go through the remaining rectangles and find the one with the highest probability, the highest $ğ‘_ğ‘$ value. Here, it is the box with $ğ‘_ğ‘$=0.8. Next, a task of ğ‘ğ‘œğ‘›âˆ’ğ‘€ğ‘ğ‘¥ ğ‘ ğ‘¢ğ‘ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘› is to discard the remaining detections having high ğ¼ğ‘œğ‘ˆ with this cell. Here, we have two bounding boxes with lower $ğ‘_ğ‘$ that recognized the car on the left (two red rectangles in the left in the above picture) and because they both have big ğ¼ğ‘œğ‘ˆ both of them will be discarded.\n",
    "\n",
    "Applying the ğ‘ğ‘œğ‘›âˆ’ğ‘€ğ‘ğ‘¥ ğ‘ ğ‘¢ğ‘ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘› means that weâ€™re going to output our maximal classifications probabilities, and suppress the other ones (the other detections of the same object) with lower values. Thatâ€™s why we use the name ğ‘ğ‘œğ‘›âˆ’ğ‘€ğ‘ğ‘¥ ğ‘ ğ‘¢ğ‘ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘›.\n",
    "\n",
    "First, within this 19Ã—19 grid weâ€™re going to get a 19Ã—19Ã—8 output volume. For each of these 361 cells we output the vector:\n",
    "\n",
    "$$\\begin{bmatrix} p_{c}\\\\b_{x}\\\\b_{y}\\\\b_{h}\\\\b_{w}\\\\c_{1}\\\\c_{2}\\\\c_{3} \\end{bmatrix}$$\n",
    "\n",
    "For this example, we will be detecting only cars. So, for every grid cell, we will obtain the following vector: \n",
    "\n",
    "$$\\begin{bmatrix} p_{c}\\\\b_{x}\\\\b_{y}\\\\b_{h}\\\\b_{w} \\end{bmatrix}$$\n",
    "\n",
    "Letâ€™s see how ğ‘ğ‘œğ‘›âˆ’ğ‘€ğ‘ğ‘¥ ğ‘ ğ‘¢ğ‘ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘› works for this example. It will first discard all the boxes with $ğ‘_ğ‘$ values less than some predefined threshold. We can use for instance a value of 0.6.  For every grid cell we output a bounding box together with a probability of detecting a car within bounding box.\n",
    "\n",
    "Next, while there are any remaining bounding boxes that we have not yet discarded or processed, we are going to repeatedly select the box with the highest $ğ‘_ğ‘$. That will be selected as our output prediction. Next, we will discard any remaining box with high overlap ( ğ¼ğ‘œğ‘ˆ) with the box that we have just outputted in the previous step. We keep doing this while there are still any remaining boxes that weâ€™ve not yet process until we have taken each of the boxes and either output it as a prediction, or discarded it. \n",
    "\n",
    "In case that we want to detect more objects, for instance: pedestrians, cars, and motorcycles, the output vector will have three $ğ‘_ğ‘$ components. It has been shown in practice that we can run ğ‘ğ‘œğ‘›âˆ’ğ‘€ğ‘ğ‘¥ ğ‘ ğ‘¢ğ‘ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘› three times independently, one for every output class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Anchor Boxes\n",
    "\n",
    "As we can see from our previous sections, object detection is quite challenging. This is the final challenge that we are going to explain. Then, we will develop a holistic YOLO algorithm.\n",
    "One scenario that we may encounter in practice is that several objects of interest are present in the same grid cell. This is shown in the figure below. In this case, we can use the idea of ğ´ğ‘›ğ‘â„ğ‘œğ‘Ÿ ğ‘ğ‘œğ‘¥ğ‘’ğ‘  to solve this problem. So, letâ€™s start with an example.\n",
    "\n",
    "<img src=\"figures/anchor_boxes.png\" style=\"width:700px\">\n",
    "\n",
    "In the figure above, we will use a 3Ã—3 grid. The midpoint of both objects, the car and the pedestrian are almost in the same place within the same grid cell. If we use the previously developed ideas, our output vector ğ‘¦ will have the following structure:\n",
    "\n",
    "\n",
    "$$y = \\begin{bmatrix} 1\\\\ b_{x}\\\\ b_{y}\\\\ b_{h}\\\\ b_{w}\\\\ 1\\\\  0\\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "Obvious challenge is that with this vector we can not detect all three desired classes: pedestrians, cars and motorcycles. That is, we canâ€™t have two detections for a single cell and we have to choose only one.\n",
    "\n",
    "The main idea of ğ‘ğ‘›ğ‘â„ğ‘œğ‘Ÿğ‘ğ‘œğ‘¥ğ‘’ğ‘  is to predefine two different shapes. They are called anchor boxes or anchor box shapes. In this way, we will be able to associate two predictions with the two anchor boxes. In general, we might use even more anchor boxes (five or even more), but to make the description easier we will stick with only two shapes.\n",
    "\n",
    "As we can see in the above picture, we defined anchor box 1 and anchor box 2. Every anchor box is defined with the following values: $ğ‘_ğ‘$,$ğ‘_ğ‘¥$,$ğ‘_ğ‘¦$,$ğ‘_â„$,$ğ‘_ğ‘¤$,ğ‘1,ğ‘2,ğ‘3. Obviously, the shape of the pedestrian is more similar to the shape of anchor box 1 and the shape of a car is more similar to the shape of anchor box 2. Hence, the vector associated with the grid cell in the middle will be:\n",
    "\n",
    "$$y =  \\begin{bmatrix} 1\\\\ b_{x}\\\\ b_{y}\\\\ b_{h}\\\\ b_{w}\\\\ 1\\\\  0\\\\ 0\\\\ 1\\\\ b_{x}\\\\ b_{y}\\\\ b_{h}\\\\ b_{w}\\\\ 0\\\\ 1\\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "Previously, before we were using ğ‘ğ‘›ğ‘ğ‘œğ‘Ÿğ‘ğ‘œğ‘¥ğ‘’ğ‘ , we defined a grid for each training image and we assigned an object to the grid cell where the center of a grid is. So, the output was 3Ã—3Ã—8 dimensional because we are using the 3Ã—3 grid and in each grid cell we have the values: $ğ‘_ğ‘$, $ğ‘_ğ‘¥$, $ğ‘_ğ‘¦$, $ğ‘_â„$, $ğ‘_ğ‘¤$, ğ‘1, ğ‘2, ğ‘3.\n",
    "\n",
    "## 9.1. How do we encode the objects in the target label?\n",
    "\n",
    "Previously, each object in the training image is assigned to a grid cell that contains that objectâ€™s midpoint. However, now with two anchor boxes, each object is assigned to grid cell that contains objectâ€™s midpoint and anchor box for the grid cell with highest Intersection over Union (ğ¼ğ‘œğ‘ˆ).\n",
    "\n",
    "Now, the output ğ‘¦ is going to be 3Ã—3Ã—16 or 3Ã—3Ã—2Ã—8 because we use now 2ğ‘ğ‘›ğ‘â„ğ‘œğ‘Ÿğ‘ğ‘œğ‘¥ğ‘’ğ‘  and ğ‘¦ is 8 dimensional. \n",
    "\n",
    "Letâ€™s go through a concrete example. For this grid cell letâ€™s specify what is ğ‘¦.\n",
    "\n",
    "<img src=\"figures/anchor_boxes_2.png\" style=\"width:700px\">\n",
    "\n",
    "Looking at this image, we see the pedestrian is more similar to the shape of Anchor box 1, so we will assign the anchor box 1 to the pedestrian. Also looking at the shape of the car, we would assign it to anchor box 2. If a car was actually found in the image, both output ğ‘1 and ğ‘3 would be 0 and ğ‘2  would be 1.\n",
    "\n",
    "Note that this algorithm will not work properly in two different cases:\n",
    "\n",
    "* When we have 2 Anchor boxes, and 3 objects in the same grid cell.\n",
    "* Also, 2 objects in the same grid cell, and both objects have the same Anchor box.\n",
    "\n",
    "These are some special cases which generally wonâ€™t happen so frequently in practice. Hence, so they do not affect the performance of the algorithm that much. It will happen quite rarely especially if we use a 19Ã—19 grid. In this case, the chance that the two objects have the same midpoint will not happen that often.\n",
    "\n",
    "## 9.2. how do we choose the anchor boxes?\n",
    "\n",
    "Normally, a simple approach to this selection process is to manually select by hand. For example, choosing 5 to 10 Anchor box shapes that spans the object we wish to detect. A more advance technique is to apply the ğ‘˜âˆ’ğ‘šğ‘’ğ‘ğ‘›ğ‘  clustering algorithm to groups together the types of object shapes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlcourse-venv",
   "language": "python",
   "name": "dlcourse-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
