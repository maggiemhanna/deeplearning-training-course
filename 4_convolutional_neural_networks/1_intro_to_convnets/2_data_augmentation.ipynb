{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure class=\"kg-card kg-image-card\"><img alt=\"cat images in different angles\" class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_C8hNiOqur4OJyEZmC7OnzQ.png\"/></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 class=\"white\">Data Augmentation | How to use Deep Learning when you have Limited Data</h1>\n",
    "    <p>We have all been there. You have a stellar concept that can be implemented using a machine learning model. Feeling ebullient, you open your web browser and search for relevant data. Chances are, you find a <strong><strong>dataset</strong></strong> that has around a <strong><strong>few hundred images</strong></strong>.</p><p>You recall that most <strong><strong>popular datasets</strong></strong> have <strong><strong>images</strong></strong> in the order of <strong><strong>tens of thousands (or more)</strong></strong>. You also recall someone mentioning having a large dataset is crucial for good performance. Feeling disappointed, you wonder; can my “state-of-the-art” neural network perform well with the meagre amount of data I have?</p><p>The answer is, <strong><strong>yes! </strong></strong>But before we get into the magic of making that happen, we need to reflect upon some basic questions.</p>\n",
    "    <h2 id=\"why-is-there-a-need-for-a-large-amount-of-data\">1. Why is there a need for a large amount of data?</h2><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"Number of parameter in popular neural networks VGGNet, DeepVideo and GNMT\" class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_1MSFA5rkyp5uJgblExWy4Q.png\" style=\"padding: 20px\"/><figcaption>Number of parameters (in millions), for popular neural networks.</figcaption></figure><p>When you <strong><strong> train a machine learning model</strong></strong>, what you’re really doing is <strong><strong>tuning its parameters</strong></strong> such that it can map a particular input (say, an image) to some output (a label). Our optimization goal is to chase that sweet spot where our model’s loss is low, which happens when your parameters are tuned in the right way.</p><blockquote><em>State of the art neural networks typically have parameters in the order of millions!</em></blockquote><p>Naturally, if you have a <strong><strong>lot of parameters</strong></strong>, you would need to show your machine learning model a <strong><strong>proportional amount of examples</strong></strong>, to get good performance. Also, the number of <strong><strong>parameters</strong></strong> you need is <strong><strong>proportional</strong></strong> to the <strong><strong>complexity</strong></strong> of the task your model has to perform.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"how-do-i-get-more-data-if-i-don-t-have-more-data-\">2. How do I get more data, if I don’t have “more data”?</h2><p>You don’t need to hunt for novel new images that can be added to your dataset. Why? Because, neural networks aren’t smart to begin with. For instance, a poorly trained neural network would think that these three tennis balls shown below, are distinct, unique images.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"Tennis ball with different positions\" class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_L07HTRw7zuHGT4oYEMlDig.jpeg\" style=\"padding:20px\"/><figcaption>The same tennis ball, but translated.</figcaption></figure><p>So, to get more data, we just need to make minor alterations to our existing dataset. Minor changes such as flips or translations or rotations. Our neural network would think these are distinct images anyway.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"application of data augmentation in deep learning layers\" class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_dJNlEc7yf93K4pjRJL55PA--1-.png\" style=\"padding:20px\"/><figcaption>Data Augmentation in play</figcaption></figure><p>A convolutional neural network that can robustly classify objects even if its placed in different orientations is said to have the property called<strong><strong> invariance</strong></strong>. More specifically, a CNN can be invariant to <strong><strong>translation, viewpoint, size</strong></strong> or <strong><strong>illumination </strong></strong>(Or a combination of the above).</p><p>This essentially is the premise of <strong><strong>data augmentation</strong></strong>. In the real world scenario, we may have a <strong><strong>dataset </strong></strong>of images taken in a <strong><strong>limited set of conditions</strong></strong>. But, our <strong><strong>target application</strong></strong> may exist in a <strong><strong>variety of conditions</strong></strong>, such as different orientation, location, scale, brightness etc. We account for these situations by training our neural network with additional <strong><strong>synthetically modified data</strong></strong>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"can-augmentation-help-even-if-i-have-lots-of-data\">3. Can augmentation help even if I have lots of data?</h2><p>Yes. It can help to increase the amount of <strong><strong>relevant data</strong></strong> in your dataset. This is related to the way with which neural networks learn. Let me illustrate it with an example.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"cars ford and chevrolet in blue colors\" class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_mvvwI7arKfLql1PoAu3www.png\" style=\"padding:20px\"/><figcaption>The two classes in our hypothetical dataset. The one in the left represents Brand A (Ford), and the one in the right represents Brand B (Chevrolet).</figcaption></figure><p>Imagine that you have a dataset, consisting of <strong><strong>two brands of cars</strong></strong>, as shown above. Let’s assume that all cars of <strong><strong>brand A</strong></strong> are aligned exactly like the picture in the left (i.e. All cars are <strong><strong>facing left</strong></strong>) . Likewise, all cars of <strong><strong>brand B</strong></strong>are aligned exactly like the picture in the right (i.e. <strong><strong>Facing right</strong></strong>) . Now, you feed this dataset to your “state-of-the-art” neural network, and you hope to get impressive results once it’s trained.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"Ford car facing right\" class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_fns3mY0dVd67RI77Bk1fCg.jpeg\" style=\"padding:20px\"/><figcaption>A Ford car (Brand A), but facing right.</figcaption></figure><p>Let’s say it’s done training, and you feed the image above, which is a Brand A car. But your neural network outputs that it’s a Brand B car! You’re confused. Didn’t you just get a 95% accuracy on your dataset using your “state-of-the-art” neural network? I’m not exaggerating, similar incidents and goof-ups have occurred in the past.</p><p><strong><strong>Why does this happen?</strong></strong> It happens because that’s how most machine learning algorithms work. It finds the most obvious features that distinguishes one class from another. Here, the feature was that all cars of Brand A were facing left, and all cars of Brand B are facing right.</p><blockquote><em>Your neural network is only as good as the data you feed it.</em></blockquote><p><strong><strong>How do we prevent this happening? </strong></strong>We have to reduce the amount of irrelevant features in the dataset. For our car model classifier above, a simple solution would be to add pictures of cars of both classes, facing the other direction to our original dataset. Better yet, you can just<strong><strong> flip</strong></strong> the <strong><strong>images</strong></strong> in the existing dataset <strong><strong>horizontally</strong></strong> such that they face the other side! Now, on training the neural network on this new dataset, you get the performance that you intended to get.</p><p><em>By performing augmentation, can prevent your neural network from learning irrelevant patterns, essentially boosting overall performance.</em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"where-do-we-augment-data-in-our-ml-pipeline\">4. Where do we augment data in our ML pipeline?</h2><p>The answer may seem quite obvious; we do augmentation before we feed the data to the model right? Yes, but you have two options here. One option is to perform all the necessary transformations beforehand, essentially increasing the size of your dataset. The other option is to perform these transformations on a mini-batch, just before feeding it to your machine learning model.</p><p>The first option is known as <strong><strong>offline augmentation</strong></strong>. This method is preferred for relatively<strong><strong> smaller datasets</strong></strong>, as you would end up increasing the size of the dataset by a factor equal to the number of transformations you perform (For example, by <strong><strong>flipping all my images</strong></strong>, I would <strong><strong>increase the size </strong></strong>of my dataset by a <strong><strong>factor of 2</strong></strong>).</p><p>The second option is known as <strong><strong>online augmentation</strong></strong>, or<strong><strong> augmentation on the fly. </strong></strong>This method is preferred for<strong><strong> larger datasets</strong></strong>, as you can’t afford the explosive increase in size. Instead, you would perform transformations on the mini-batches that you would feed to your model. Some machine learning frameworks have support for online augmentation, which can be accelerated on the GPU.</p><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"popular-augmentation-techniques\">5. Popular Augmentation Techniques</h2><p>In this section, we present some basic but powerful augmentation techniques that are popularly used. Before we explore these techniques, <strong><strong>for simplicity</strong></strong>, let us make <strong><strong>one assumption</strong></strong>. The assumption is that, <strong><strong>we don’t need to consider what lies beyond the image’s boundary</strong></strong>. We’ll use the below techniques such that our assumption is valid.</p><p>What would happen if we use a technique that forces us to guess what lies beyond an image’s boundary? In this case, we need to <strong><strong>interpolate</strong></strong> some information. We’ll discuss this in detail after we cover the types of augmentation.</p><p>For each of these techniques, we also specify the factor by which the size of your dataset would get increased (aka. Data Augmentation Factor).</p><h3 id=\"1-flip\">5.1. Flip</h3><p>You can flip images horizontally and vertically. Some frameworks do not provide function for vertical flips. But, a vertical flip is equivalent to rotating an image by 180 degrees and then performing a horizontal flip. Below are examples for images that are flipped.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"stressball in blue and yellow colours with flipped angles\" class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_-beH1nNqlm_Wj-0PcWUKTw.jpeg\" style=\"padding: 20px\"/><figcaption>From the left, we have the original image, followed by the image flipped horizontally, and then the image flipped vertically.</figcaption></figure><p>You can perform flips by using any of the following commands, from your favorite packages. <strong><strong>Data Augmentation Factor = 2 to 4x</strong></strong></p><pre>\n",
    "\n",
    "```python\n",
    "You can perform flips by using any of the following commands, from your favorite packages. Data Augmentation Factor = 2 to 4\n",
    "# NumPy.'img' = A single image.\n",
    "flip_1 = np.fliplr(img)\n",
    "# TensorFlow. 'x' = A placeholder for an image.\n",
    "shape = [height, width, channels]\n",
    "x = tf.placeholder(dtype = tf.float32, shape = shape)\n",
    "flip_2 = tf.image.flip_up_down(x)\n",
    "flip_3 = tf.image.flip_left_right(x)\n",
    "flip_4 = tf.image.random_flip_up_down(x)\n",
    "flip_5 = tf.image.random_flip_left_right(x)\n",
    "```\n",
    "\n",
    "</pre><h3 id=\"2-rotation\">5.2. Rotation</h3><p>One key thing to note about this operation is that image dimensions may not be preserved after rotation. If your image is a square, rotating it at right angles will preserve the image size. If it’s a rectangle, rotating it by 180 degrees would preserve the size. Rotating the image by finer angles will also change the final image size. We’ll see how we can deal with this issue in the next section. Below are examples of square images rotated at right angles.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"stressball rotated in different angles for data augmentation\" class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_i_F6aNKj3yggkcNXQxYA4A.jpeg\" style=\"padding: 20px\"/><figcaption>The images are rotated by 90 degrees clockwise with respect to the previous one, as we move from left to right.</figcaption></figure><p>You can perform rotations by using any of the following commands, from your favorite packages. <strong><strong>Data Augmentation Factor = 2 to 4x</strong></strong></p><pre>\n",
    "\n",
    "\n",
    "```python\n",
    "# Placeholders: 'x' = A single image, 'y' = A batch of images\n",
    "# 'k' denotes the number of 90 degree anticlockwise rotations\n",
    "shape = [height, width, channels]\n",
    "x = tf.placeholder(dtype = tf.float32, shape = shape)\n",
    "rot_90 = tf.image.rot90(img, k=1)\n",
    "rot_180 = tf.image.rot90(img, k=2)\n",
    "# To rotate in any angle. In the example below, 'angles' is in radians\n",
    "shape = [batch, height, width, 3]\n",
    "y = tf.placeholder(dtype = tf.float32, shape = shape)\n",
    "rot_tf_180 = tf.contrib.image.rotate(y, angles=3.1415)\n",
    "# Scikit-Image. 'angle' = Degrees. 'img' = Input Image\n",
    "# For details about 'mode', checkout the interpolation section below.\n",
    "rot = skimage.transform.rotate(img, angle=45, mode='reflect')\n",
    "```\n",
    "\n",
    "</pre>\n",
    "\n",
    "<h3 id=\"3-scale\">5.3. Scale</h3><p>The image can be scaled outward or inward. While scaling outward, the final image size will be larger than the original image size. Most image frameworks cut out a section from the new image, with size equal to the original image. We’ll deal with scaling inward in the next section, as it reduces the image size, forcing us to make assumptions about what lies beyond the boundary. Below are examples or images being scaled.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"scaled images\" class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_INLTn7GWM-m69GUwFzPOaQ.jpeg\" style=\"padding: 20px\"/><figcaption>From the left, we have the original image, the image scaled outward by 10%, and the image scaled outward by 20%</figcaption></figure><p>You can perform scaling by using the following commands, using scikit-image. <strong><strong>Data Augmentation Factor = Arbitrary.</strong></strong></p><pre>\n",
    "\n",
    "```python\n",
    "# Scikit Image. 'img' = Input Image, 'scale' = Scale factor\n",
    "# For details about 'mode', checkout the interpolation section below.\n",
    "scale_out = skimage.transform.rescale(img, scale=2.0, mode='constant')\n",
    "scale_in = skimage.transform.rescale(img, scale=0.5, mode='constant')\n",
    "# Don't forget to crop the images back to the original size (for \n",
    "# scale_out)\n",
    "```\n",
    "\n",
    "</pre>\n",
    "\n",
    "<h3 id=\"4-crop\">5.4. Crop</h3><p>Unlike scaling, we just randomly sample a section from the original image. We then resize this section to the original image size. This method is popularly known as random cropping. Below are examples of random cropping. If you look closely, you can notice the difference between this method and scaling.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_ypuimiaLtg_9KaQwltrxJQ.jpeg\" style=\"padding: 20px\"/><figcaption>From the left, we have the original image, a square section cropped from the top-left, and then a square section cropped from the bottom-right. The cropped sections were resized to the original image size.</figcaption></figure><p>You can perform random crops by using any the following command for TensorFlow. <strong><strong>Data Augmentation Factor = Arbitrary.</strong></strong></p>\n",
    "<pre>\n",
    "\n",
    "```python\n",
    "# TensorFlow. 'x' = A placeholder for an image.\n",
    "original_size = [height, width, channels]\n",
    "x = tf.placeholder(dtype = tf.float32, shape = original_size)\n",
    "# Use the following commands to perform random crops\n",
    "crop_size = [new_height, new_width, channels]\n",
    "seed = np.random.randint(1234)\n",
    "x = tf.random_crop(x, size = crop_size, seed = seed)\n",
    "output = tf.images.resize_images(x, size = original_size)\n",
    "```\n",
    "\n",
    "</pre>\n",
    "\n",
    "<h3 id=\"5-translation\">5.5. Translation</h3><p>Translation just involves moving the image along the X or Y direction (or both). In the following example, we assume that the image has a black background beyond its boundary, and are translated appropriately. This method of augmentation is very useful as most <strong><strong>objects</strong></strong> can be located at <strong><strong>almost anywhere </strong></strong>in the image. This<strong><strong> forces</strong></strong> your <strong><strong>convolutional neural network to look everywhere</strong></strong>.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"image translated to the right\" class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_L07HTRw7zuHGT4oYEMlDig(1).jpeg\" style=\"padding: 20px\"/><figcaption>From the left, we have the original image, the image translated to the right, and the image translated upwards.</figcaption></figure><p>You can perform translations in TensorFlow by using the following commands. <strong><strong>Data Augmentation Factor = Arbitrary.</strong></strong></p><pre>\n",
    "\n",
    "\n",
    "```python\n",
    "# pad_left, pad_right, pad_top, pad_bottom denote the pixel \n",
    "# displacement. Set one of them to the desired value and rest to 0\n",
    "shape = [batch, height, width, channels]\n",
    "x = tf.placeholder(dtype = tf.float32, shape = shape)\n",
    "# We use two functions to get our desired augmentation\n",
    "x = tf.image.pad_to_bounding_box(x, pad_top, pad_left, height + pad_bottom + pad_top, width + pad_right + pad_left)\n",
    "output = tf.image.crop_to_bounding_box(x, pad_bottom, pad_right, height, width)\n",
    "```\n",
    "\n",
    "</pre><h3 id=\"6-gaussian-noise\">5.6. Gaussian Noise</h3><p>Over-fitting usually happens when your neural network tries to learn high frequency features (patterns that occur a lot) that may not be useful. Gaussian noise, which has zero mean, essentially has data points in all frequencies, effectively distorting the high frequency features. This also means that lower frequency components (usually, your intended data) are also distorted, but your neural network can learn to look past that. Adding just the right amount of noise can enhance the learning capability.</p><p>A toned down version of this is the salt and pepper noise, which presents itself as random black and white pixels spread through the image. This is similar to the effect produced by adding Gaussian noise to an image, but may have a lower information distortion level.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"images with gaussian noise and salt and pepper noise\" class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_cx24OpSNOwgg7ULUHKiGnA.png\"/><figcaption>From the left, we have the original image, image with added Gaussian noise, image with added salt and pepper noise</figcaption></figure><p>You can add Gaussian noise to your image by using the following command, on TensorFlow. <strong><strong>Data Augmentation Factor = 2x.</strong></strong></p><pre>\n",
    "\n",
    "```python\n",
    "#TensorFlow. 'x' = A placeholder for an image.\n",
    "shape = [height, width, channels]\n",
    "x = tf.placeholder(dtype = tf.float32, shape = shape)\n",
    "# Adding Gaussian noise\n",
    "noise = tf.random_normal(shape=tf.shape(x), mean=0.0, stddev=1.0,\n",
    "dtype=tf.float32)\n",
    "output = tf.add(x, noise)\n",
    "```\n",
    "\n",
    "</pre>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"a-brief-note-on-interpolation\">6. A brief note on interpolation</h2><p>What if you wanted to translate an image that doesn’t have a black background? What if you wanted to scale inward? Or rotate in finer angles? After we perform these transformations, we need to preserve our original image size. Since our image does not have any information about things outside it’s boundary, we need to make some assumptions. Usually, the space beyond the image’s boundary is assumed to be the constant 0 at every point. Hence, when you do these transformations, you get a black region where the image is not defined.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_z8_8gq5zgA_9peaTfyx2gQ.jpeg\" style=\"padding:20px\"/><figcaption>From the left, an image rotated by 45 degrees anticlockwise, an image translated to the right, and an image scaled inward.</figcaption></figure><p><strong><strong>But is that the right assumption? </strong></strong>In the real world scenario, it’s mostly a no. Image processing and ML frameworks have some standard ways with which you can decide on how to fill the unknown space. They are defined as follows.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_rG4YJyVdu28LZwkwkwC55w.jpeg\" style=\"padding:20px\"/><figcaption>From the left, we have the constant, edge, reflect, symmetric and wrap modes.</figcaption></figure><h3 id=\"1-constant\">6.1. Constant</h3><p>The simplest interpolation method is to fill the unknown region with some constant value. This may not work for natural images, but can work for images taken in a monochromatic background</p><h3 id=\"2-edge\">6.2. Edge</h3><p>The edge values of the image are extended after the boundary. This method can work for mild translations.</p><h3 id=\"3-reflect\">6.3. Reflect</h3><p>The image pixel values are reflected along the image boundary. This method is useful for continuous or natural backgrounds containing trees, mountains etc.</p><h3 id=\"4-symmetric\">6.4. Symmetric</h3><p>This method is similar to reflect, except for the fact that, at the boundary of reflection, a copy of the edge pixels are made. Normally, reflect and symmetric can be used interchangeably, but differences will be visible while dealing with very small images or patterns.</p><h3 id=\"5-wrap\">6.5. Wrap</h3><p>The image is just repeated beyond its boundary, as if it’s being tiled. This method is not as popularly used as the rest as it does not make sense for a lot of scenarios.</p><p>Besides these, you can design your own methods for dealing with undefined space, but usually these methods would just do fine for most classification problems.</p><hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"so-if-i-use-all-of-these-techniques-my-ml-algorithm-would-be-robust-right\">7. So, if I use ALL of these techniques, my ML algorithm would be robust right?</h2><p>If you use it in the <strong><strong>right way</strong></strong>, then yes! What is the right way you ask? Well, sometimes not all augmentation techniques make sense for a dataset. Consider our car example again.</p><p>Below are some of the ways by which you can modify the image.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"cars flipped in different angles for dataset\" class=\"kg-image lightense-target\" src=\"./data_augmentation_files/1_vW3KGPp_w0wN6k3gYVlVHA.jpeg\" style=\"padding:20px\"/><figcaption>The first image (from the left) is the original, the second one is flipped horizontally, the third one is rotated by 180 degrees, and the last one is rotated by 90 degrees (clockwise).</figcaption></figure><p>Sure, they are pictures of the same car, but your <strong><strong>target application may never see cars presented in these orientations</strong></strong>.For instance, if you’re just going to classify random cars on the road, only the second image would make sense to be on the dataset. But, if you own an insurance company that deals with car accidents, and you want to identify models of upside-down, broken cars as well, the third image makes sense. The last image may not make sense for both the above scenarios.The point is, while using augmentation techniques, we have to make sure to <strong><strong>not increase irrelevant data</strong></strong>.</p><hr/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlcourse-venv",
   "language": "python",
   "name": "dlcourse-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
