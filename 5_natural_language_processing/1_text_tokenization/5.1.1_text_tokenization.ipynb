{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zX4Kg8DUTKWO"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course we're going to go back to building models but we'll focused on text and how you can build classifier is based on text models. We'll start by looking at sentiment in text, and learn how to build models that understand text that are trained on labeled text, and then can then classify new text based on what they've seen.\n",
    "\n",
    "When we were dealing with images, it was relatively easy for us to feed them into a neural network, as the pixel values were already numbers. And the network could learn parameters of functions that could be used to fit classes to labels. But what happens with text? How can we do that with sentences and words?\n",
    "\n",
    "We could take character encodings for each character in a set. For example, the ASCII values. But will that help us understand the meaning of a word? So for example, consider the word 'LISTEN' as shown here. A common simple character encoding is ASCII, the American Standard Code for Information Interchange with the values as shown here. So you might think you could have a word like LISTEN encoded using these values. But the problem with this of course, is that the semantics of the word aren't encoded in the letters. This could be demonstrated using the word 'SILENT ' which has a very different and almost opposite meaning, but with exactly the same letters. So it seems that training a neural network with just the letters could be a daunting task.\n",
    "\n",
    "<img src=\"figures/char-token.png\" style=\"width:500px\">\n",
    "\n",
    "So how about if we consider words? What if we could give words a value and have those values used in training a network? Now we could be getting somewhere. So for example, consider this sentence, I Love my dog. How about giving a value to each word? What that value is doesn't matter. It's just that we have a value per word, and the value is the same for the same word every time. So a simple encoding for the sentence would be for example to give word 'I' the value one. Following on, we could give the words 'Love', 'my' and 'dog' the values 2, 3, and 4 respectively. So then the sentence, I love my dog would be encoded as 1, 2, 3, 4. So now, what if I have the sentence, I love my cat? Well, we've already encoded the words 'I love my' as 1, 2, 3. So we can reuse those, and we can create a new token for cat, which we haven't seen before. So let's make that the number 5. Now if we just look at the two sets of encodings, we can begin to see some similarity between the sentences. I love my dog is 1, 2, 3, 4 and I love my cat is 1, 2, 3, 5. So this is at least a beginning and how we can start training a neural network based on words. \n",
    "\n",
    "<img src=\"figures/word-token.png\" style=\"width:500px\">\n",
    "\n",
    "\n",
    "Fortunately, TensorFlow and Care Ask give us some APIs that make it very simple to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/text-to-seq.png\" style=\"width:600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"\\nWord Index = \" , word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(\"\\nSequences = \" , sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded Sequences:\n",
      "[[ 0  5  3  2  4]\n",
      " [ 0  5  3  2  7]\n",
      " [ 0  6  3  2  4]\n",
      " [ 9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, maxlen=5)â€¡\n",
    "\n",
    "print(\"\\nPadded Sequences:\")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArOPfBwyZtln"
   },
   "outputs": [],
   "source": [
    "# Try with words that the tokenizer wasn't fit to\n",
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "padded = pad_sequences(test_seq, maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Course 3 - Week 1 - Lesson 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "dlcourse-venv",
   "language": "python",
   "name": "dlcourse-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
