{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What Are Word Embeddings?\n",
    "\n",
    "A word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n",
    "\n",
    "It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems.\n",
    "\n",
    ">One of the benefits of using dense and low-dimensional vectors is computational: the majority of neural network toolkits do not play well with very high-dimensional, sparse vectors. … The main benefit of the dense representations is generalization power: if we believe some features may provide similar clues, it is worthwhile to provide a representation that is able to capture these similarities.\n",
    "\n",
    "## 1.1. One Hot Encoding\n",
    "\n",
    "One-hot encoding may be one’s go-to move when a categorical feature is encountered in a data set(if you think such a feature is useful for the model, obviously). It’s a simple and straight forward technique and it works by replacing each category with a vector full of zeros, except for the position of its corresponding index value, which has a value of 1.\n",
    "\n",
    "When applying one-hot encoding to a text document, tokens are replaced by their one-hot vectors and a given sentence is in turn transformed into a 2D-matrix with a shape of (n, m), with n being the number of token in the sentence and m the size of the vocabulary. \n",
    "\n",
    "<img src=\"figures/words_ohe.png\" width=\"600px\">\n",
    "\n",
    "\n",
    "```\n",
    "As an example, say that we were to use one-hot encoding for the following sentences:\n",
    "\n",
    "s1. \"This is sentence one.\"\n",
    "s2. \"Now, here is our sentence number two.\"\n",
    "\n",
    "The vocabulary from the two sentences is:\n",
    "vocabulary = {\"here\": 0, \"is\": 1, \"now\": 2, \"number\": 3, \"one\": 4, \"our\": 5, \"sentence\": 6, \"this\": 7, \"two\": 8}\n",
    "The two sentences represented by one-hot vectors are:\n",
    "indices                             words\n",
    "      0  1  2  3  4  5  6  7  8\n",
    "s1: [[0, 0, 0, 0, 0, 0, 0, 1, 0], - \"this\"\n",
    "     [0, 1, 0, 0, 0, 0, 0, 0, 0], - \"is\"\n",
    "     [0, 0, 0, 0, 0, 0, 1, 0, 0], - \"sentence\"\n",
    "     [0, 0, 0, 0, 1, 0, 0, 0, 0], - \"one\"\n",
    "     [0, 0, 0, 0, 0, 0, 0, 0, 0], - PAD\n",
    "     [0, 0, 0, 0, 0, 0, 0, 0, 0], - PAD\n",
    "     [0, 0, 0, 0, 0, 0, 0, 0, 0]] - PAD\n",
    "     \n",
    "s2: [[0, 0, 1, 0, 0, 0, 0, 0, 0], - \"now\"\n",
    "     [1, 0, 0, 0, 0, 0, 0, 0, 0], - \"here\"\n",
    "     [0, 1, 0, 0, 0, 0, 0, 0, 0], - \"is\"\n",
    "     [0, 0, 0, 0, 0, 1, 0, 0, 0], - \"our\"\n",
    "     [0, 0, 0, 0, 0, 0, 1, 0, 0], - \"sentence\"\n",
    "     [0, 0, 0, 1, 0, 0, 0, 0, 0], - \"number\"\n",
    "     [0, 0, 0, 0, 0, 0, 0, 0, 1]] - \"two\"\n",
    "```\n",
    "\n",
    "The size of the vocabulary would only grow as the training corpus gets larger and larger, as a result, each token would be represented by vectors with an increasingly larger length, making matrices more sparse. \n",
    "\n",
    "For NLP tasks such as Text Generation or Classification, one-hot representation or count vectors might be capable enough to represent the required information for the model to make wise decisions. However, their usage won’t be as effective for other tasks such as Sentiment Analysis, Neural Machine Translation, and Question Answering where a deeper understanding of the context is required to achieve great results.\n",
    "Take One-hot encoding as an example, using it will not lead to a well-generalized model for these tasks because no comparison between any two given words can be done. All vectors are orthogonal to one another, the inner product of any two vectors is zero and their similarities cannot be measure by distance.\n",
    "\n",
    "## 1.2. Embeddings\n",
    "\n",
    "For this, we turn to Word Embeddings, a featurized word-level representation capable of capturing the semantic meanings of words.\n",
    "With embeddings, each word is represented by a dense vector of fixed size(generally range from 50 to 300), with values corresponding to a set of features i.e. Masculinity, Femininity, Age, etc. As shown in the figure below, these features are seen as different aspects of a word’s semantic meaning, and their values are obtained by random initialization and are updated during training, just like the parameters of the model do.\n",
    "\n",
    "<img src=\"figures/words_embeddings.png\">\n",
    "\n",
    "When training embeddings, we do not tell the model what these features should be, instead it’s up to the model to decide what are the best ones for the learning task. When setting up an embedding matrix(a set of word embeddings), we only define its shape — the number of words and each vector’s length. What each feature is representing is generally difficult to interpret.\n",
    "\n",
    "Word Embedding’s ability to capture semantic meanings can be illustrated by projecting these high-dimensional vectors to a 2D space for visualization via t-SNE. If embeddings were obtained successfully, plotting these vectors with t-SNE would demonstrate how words that have similar meaning would end up being closer to one another.\n",
    "\n",
    "<img src=\"figures/words_embeddings_2D.png\" width=\"600px\">\n",
    "\n",
    "## 1.3. Training Word Embeddings\n",
    "\n",
    "Word embedding methods learn a real-valued vector representation for a predefined fixed sized vocabulary from a corpus of text.\n",
    "\n",
    "The learning process is either joint with the neural network model on some task, such as document classification, or is an unsupervised process, using document statistics.\n",
    "\n",
    "<img src=\"figures/catagorical_embedding.svg\" width=\"800px\">\n",
    "\n",
    "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset (could also be used with sparse catagorical inputs).\n",
    "\n",
    "It is a flexible layer that can be used in a variety of ways, such as:\n",
    "\n",
    "It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
    "It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
    "It can be used to load a pre-trained word embedding model, a type of transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TF Keras Embedding Layer\n",
    "\n",
    "Keras offers an Embedding layer that can be used for neural networks on text data.\n",
    "\n",
    "**It requires that the input data be integer encoded** (tf keras has an efficient way of transforming integer encodings to embeddings instead of mapping the one hot sparse vector encoding to dense representation), so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
    "\n",
    "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
    "\n",
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "It must specify 3 arguments:\n",
    "\n",
    "* input_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "* output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "* input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "\n",
    "The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document).\n",
    "\n",
    "If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. How embedding layer works in TF Keras?\n",
    "\n",
    "Embedding layer performs select operation for each integer input that is used as the index to access a table that contains all possible vectors.\n",
    "\n",
    "<img src=\"figures/embedding_layer.jpeg\" >\n",
    "\n",
    "\n",
    "\n",
    "Let’s see a simple text processing example. Our training set consists only of two phrases:\n",
    "\n",
    "Hope to see you soon → [0, 1, 2, 3, 4]\n",
    "\n",
    "Nice to see you again → [5, 1, 2, 3, 6]\n",
    "\n",
    "So we encoded these phrases by assigning each word a unique integer number (by order of appearance in our training dataset for example) as the index of the embedding matrix. That is the reason below, why you need to specify the size of the vocabulary as the first argument (so the table can be initialised).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.00717581 -0.01187963]\n",
      "  [-0.04137641 -0.00176752]\n",
      "  [ 0.02428936  0.02234766]\n",
      "  [-0.02888275 -0.0383324 ]\n",
      "  [-0.02276908 -0.04999352]]\n",
      "\n",
      " [[ 0.0312614  -0.01271391]\n",
      "  [-0.04137641 -0.00176752]\n",
      "  [ 0.02428936  0.02234766]\n",
      "  [-0.02888275 -0.0383324 ]\n",
      "  [ 0.00499725 -0.02520583]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "X = np.array([\n",
    "    [0, 1, 2, 3, 4],\n",
    "    [5, 1, 2, 3, 6]\n",
    "])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=7,\n",
    "                    output_dim=2,\n",
    "                    input_length=5))\n",
    "\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "print(model.predict(X, batch_size=32))  # output (2, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding/embeddings:0' shape=(7, 2) dtype=float32, numpy=\n",
       " array([[-0.00717581, -0.01187963],\n",
       "        [-0.04137641, -0.00176752],\n",
       "        [ 0.02428936,  0.02234766],\n",
       "        [-0.02888275, -0.0383324 ],\n",
       "        [-0.02276908, -0.04999352],\n",
       "        [ 0.0312614 , -0.01271391],\n",
       "        [ 0.00499725, -0.02520583]], dtype=float32)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can emulate an embedding layer with fully-connected (Dense) layer via converting every integer input into one-hot encoding and performing matrix multiplication with the weights(embedding matrix).**\n",
    "\n",
    "The whole point of dense embedding is to avoid one-hot representation. In NLP, the vocabulary size can be of the order 100k (sometimes even a million). On top of that, it’s often needed to process the sequences of words in a batch. Processing the batch of sequences of word indices would be much more efficient than the batch of sequences of one-hot vectors. In addition, gather operation itself is faster than matrix dot-product, both in forward and backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. How the embedding layer is trained in Keras?\n",
    "\n",
    "Embedding layers in Keras are trained just like any other layer in your network architecture: they are tuned to minimize the loss function by using the selected optimization method. The major difference with other layers, is that their output is not a mathematical function of the input. Instead the input to the layer is used to index a table with the embedding vectors as we have seen above. However, the underlying automatic differentiation engine treats embeddings as any other weights in network and optimize these vectors to minimize the loss function.\n",
    "\n",
    "There are very specific network setups that try to learn an embedding on a supervised task like word2vec, captures the semantics of words, or sentiment classification problems. Let’s implement a small binary classification task by visualisation the details of the architecture to gain deeper level of understanding.\n",
    "\n",
    "We will have 10 text documents, each with a comment about a piece of work a student submitted. Each text document is classified as positive “1” or negative “0”.\n",
    "\n",
    "<img src=\"figures/embedding_layer_learning.jpeg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 4, 4)              80        \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 97\n",
      "Trainable params: 97\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6886 - accuracy: 0.6000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6877 - accuracy: 0.6000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6868 - accuracy: 0.6000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 922us/step - loss: 0.6859 - accuracy: 0.6000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6850 - accuracy: 0.6000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6841 - accuracy: 0.7000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6831 - accuracy: 0.8000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6822 - accuracy: 0.8000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6813 - accuracy: 0.8000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6804 - accuracy: 0.8000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6794 - accuracy: 0.8000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6785 - accuracy: 0.8000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6776 - accuracy: 0.9000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6766 - accuracy: 0.9000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 982us/step - loss: 0.6757 - accuracy: 0.9000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6747 - accuracy: 0.9000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6738 - accuracy: 0.9000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.9000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6719 - accuracy: 0.9000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6709 - accuracy: 0.9000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6699 - accuracy: 0.9000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6689 - accuracy: 0.9000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6680 - accuracy: 0.9000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6670 - accuracy: 0.9000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6660 - accuracy: 0.9000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6650 - accuracy: 0.9000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6640 - accuracy: 0.9000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6630 - accuracy: 0.9000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6619 - accuracy: 0.9000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6609 - accuracy: 0.9000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6599 - accuracy: 0.9000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6588 - accuracy: 0.9000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6578 - accuracy: 0.9000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6567 - accuracy: 0.9000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6557 - accuracy: 0.9000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6546 - accuracy: 0.9000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6535 - accuracy: 0.9000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6525 - accuracy: 0.9000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6514 - accuracy: 0.9000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6503 - accuracy: 0.9000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6492 - accuracy: 0.9000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6481 - accuracy: 0.9000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6470 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6458 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6447 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 911us/step - loss: 0.6436 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6424 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6413 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6401 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6390 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6378 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6366 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6354 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 829us/step - loss: 0.6343 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6331 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6319 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6306 - accuracy: 1.00 - 0s 1ms/step - loss: 0.6306 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6294 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6282 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6270 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6257 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6245 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6232 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6220 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6207 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6195 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6182 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6169 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 994us/step - loss: 0.6156 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 827us/step - loss: 0.6143 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 981us/step - loss: 0.6130 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 899us/step - loss: 0.6117 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6104 - accuracy: 1.0000\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6091 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6078 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6064 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6051 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6038 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6024 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6011 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5997 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5983 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5970 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5956 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5942 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5928 - accuracy: 1.00 - 0s 2ms/step - loss: 0.5928 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5915 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5901 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 959us/step - loss: 0.5887 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5873 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5859 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5844 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5830 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5816 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 879us/step - loss: 0.5802 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5787 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5773 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5759 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5744 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5730 - accuracy: 1.0000\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x147e361e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "\n",
    "# tokenize documents\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "word2idx = tokenizer.word_index\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = [[word2idx[w] for w in text.text_to_word_sequence(doc)] for doc in docs]\n",
    "print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "vocab_size = 20 \n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 4, input_length=max_length ))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    " \n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=100, verbose=1 )\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pre-trained Embeddings\n",
    "\n",
    "## 5.1. Word2Vec\n",
    "\n",
    "\n",
    "Word2Vec is a statistical method for efficiently learning a standalone word embedding from a text corpus.\n",
    "\n",
    "It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make the neural-network-based training of the embedding more efficient and since then has become the de facto standard for developing pre-trained word embedding.\n",
    "\n",
    "Additionally, the work involved analysis of the learned vectors and the exploration of vector math on the representations of words. For example, that subtracting the “man-ness” from “King” and adding “women-ness” results in the word “Queen“, capturing the analogy “king is to queen as man is to woman“.\n",
    "\n",
    "We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King – Man + Woman” results in a vector very close to “Queen.”\n",
    "\n",
    "\n",
    "Two different learning models were introduced that can be used as part of the word2vec approach to learn the word embedding; they are:\n",
    "\n",
    "* **Continuous Bag-of-Words Model** which predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n",
    "* **Continuous Skip-gram Model** which predict words within a certain range before and after the current word in the same sentence. A worked example of this is given below.\n",
    "\n",
    "\n",
    "<img src=\"figures/word2vec.png\">\n",
    "\n",
    "Both models are focused on learning about words given their local usage context, where the context is defined by a window of neighboring words. This window is a configurable parameter of the model.\n",
    "\n",
    "The size of the sliding window has a strong effect on the resulting vector similarities. Large windows tend to produce more topical similarities, while smaller windows tend to produce more functional and syntactic similarities.\n",
    "\n",
    "While a bag-of-words model predicts a word given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). The context of a word can be represented through a set of skip-gram pairs of (target_word, context_word) where context_word appears in the neighboring context of target_word.\n",
    "\n",
    "Consider the following sentence of 8 words.\n",
    "\n",
    ">The wide road shimmered in the hot sun.\n",
    "\n",
    "The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a target_word that can be considered context word. Take a look at this table of skip-grams for target words based on different window sizes.\n",
    "\n",
    "<img src=\"figures/skip_grams.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. GloVe\n",
    "\n",
    "\n",
    "* GloVe is another algorithm for learning the word embedding. It's the simplest of them.\n",
    "\n",
    "* This is not used as much as word2vec or skip-gram models, but it has some enthusiasts because of its simplicity.\n",
    "\n",
    "* GloVe stands for Global vectors for word representation.\n",
    "\n",
    "* Let's use this example: \"I want a glass of orange juice to go along with my cereal\".\n",
    "\n",
    "* We will choose a context and a target from the choices we have mentioned in the previous sections.\n",
    "\n",
    "* Then we will calculate this for every pair: Xct = # times t appears in context of c\n",
    "\n",
    "* Xct = Xtc if we choose a window pair, but they will not equal if we choose the previous words for example. In GloVe they use a window which means they are equal\n",
    "\n",
    "* The model is defined like this:\n",
    "\n",
    "<img src=\"figures/glove.png\" width=\"800px\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlcourse-venv",
   "language": "python",
   "name": "dlcourse-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
